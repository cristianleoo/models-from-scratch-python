{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recursive Language Models: The End of Context Rot\n",
                "\n",
                "How to escape the \"Context Window\" trap by teaching models to read like humans.\n",
                "\n",
                "It feels like the \"Context Wars\" are over. In 2023, we fought for 32k context windows. By 2024, 128k was standard. Today, Gemini, Anthropic, and many other providers give you 1 Million tokens. You can literally dump the entire Harry Potter series, the Linux Kernel, and your company's entire Slack history into a single prompt.\n",
                "\n",
                "So, problem solved, right?\n",
                "\n",
                "Wrong.\n",
                "\n",
                "While we solved the Storage problem (fitting the text in RAM), we haven't solved the Reasoning problem. This is a phenomenon known as Context Rot (or the \"Lost in the Middle\" phenomenon).\n",
                "\n",
                "As you fill that 1M window, the model gets \"dumber\". Its attention mechanism—the very heart of the Transformer—becomes diluted. When you ask a question about line 14,032, a model looking at 1,000,000 tokens struggles to separate the signal from the noise. It's like trying to find a specific needle in a haystack, but the haystack is the size of Texas.\n",
                "\n",
                "We don't need bigger windows. We need a fundamental shift in how models interact with data. We need them to stop \"ingesting\" data and start \"reading\" it.\n",
                "\n",
                "This is where Recursive Language Models (RLMs) come in.\n",
                "\n",
                "In this deep dive, based on the recent paper from MIT OASYS, we aren't just going to explain the theory. We are going to build an RLM from scratch."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Before running this notebook, please make sure you have your API key set up.\n",
                "\n",
                "You can export it in your terminal before running Jupyter:\n",
                "```bash\n",
                "export GOOGLE_API_KEY=your_api_key_here\n",
                "```\n",
                "\n",
                "Or set it directly in the cell below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install google-genai\n",
                "\n",
                "import os\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## From \"Seeing\" to \"Seeking\"\n",
                "\n",
                "To understand the breakthrough, we have to look at the math.\n",
                "\n",
                "In a standard Language Model (like GPT), the probability of the next word ($y$) depends on everything that came before it ($x$).\n",
                "\n",
                "This $x$ represents your massive 1-million-token prompt. The core mechanism of the Transformer, Self-Attention, has to compare every token to every other token to find relationships. This leads to a complexity of roughly $O(N^2)$.\n",
                "\n",
                "Think of this like a librarian who, every time you ask a question, has to re-read every single book in the library simultaneously before answering.\n",
                "\n",
                "Going back to the librarian example, when the library grows ($N \\to \\infty$), the librarian becomes overwhelmed. The \"signal\" of the answer gets drowned out by the \"noise\" of millions of irrelevant words. This is Context Rot."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Active Information Retrieval\n",
                "\n",
                "Recursive Language Models flip this logic. Instead of forcing the model to *see* the text, we *hide* the text. We treat the context $x$ as an Environment State—like a database or a file system—rather than input tokens.\n",
                "\n",
                "The model doesn't generate the answer immediately. It generates a Program ($\\pi$).\n",
                "\n",
                "Here is the narrative arc of this formula:\n",
                "1. $S_0$ is the user's prompt (e.g., \"Find the secret code\").\n",
                "2. The model cannot *see* the context $x$. It only knows \"There is a variable named `context` in memory\".\n",
                "3. Because it can't answer yet, $P_\\theta$ (the model) decides to write a program $\\pi$ to go look for information.\n",
                "4. This program $\\pi$ executes in the environment to produce an Observation ($o$).\n",
                "5. Finally, the model looks at its own program and the result it got back ($o$), and decides what to do next.\n",
                "\n",
                "Why is this revolutionary? Because the model's \"working memory\" (what it pays attention to) is now just the Program $\\pi$ and the Observation $o$.\n",
                "\n",
                "- **Old Way**: The model must attend to 1,000,000 tokens simultaneously ($O(N^2)$ cognitive load).\n",
                "- **RLM Way**: The model attends only to the specific chunk it requested (e.g., 500 tokens). While the system might scan the whole library, the model never holds the entire load at once.\n",
                "\n",
                "We have decoupled the Size of the Data from the Cognitive Load of the model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The \"Recursive\" in RLM\n",
                "\n",
                "The \"Recursive\" in RLM isn't just a buzzword. It literally means the model can call itself.\n",
                "\n",
                "Imagine asking the model to \"Summarize this 10,000 page document.\" A standard RLM might try to read the first 100 pages using code. But that's still too much work.\n",
                "\n",
                "In a Recursive system, the generated program $\\pi$ can invoke the RLM function again:\n",
                "- $M_d$: The Model at recursion depth $d$.\n",
                "- $M_{d-1}$: A sub-agent called by the parent.\n",
                "\n",
                "### Example Flow\n",
                "1. **Manager Agent (Depth 0)**: \"This document is huge. I'll split it into 3 parts and hire 3 sub-agents.\"\n",
                "   - Code: `for part in parts: rlm.completion(\"Summarize this\", part)`\n",
                "2. **Worker Agent A (Depth 1)**: \"I received Part 1. It's still big, but I can read it.\"\n",
                "   - Code: `read(part); summarize()`\n",
                "3. **Manager Agent**: Receives 3 summaries, combines them, and answers.\n",
                "\n",
                "This turns a linear processing task into a Tree Search. The model dynamically creates its own \"org chart\" of sub-agents to handle the complexity."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## A Simple Implementation\n",
                "\n",
                "Let's turn this math into Python. We need to build a system where the model can write code that references itself.\n",
                "\n",
                "**Note**: For clarity and simplicity, we execute Python code in a local environment. In production, this \"Environment\" would be a secure Docker container to prevent the model from deleting your files!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: The Environment\n",
                "This is our \"Computer\". It stores variables and runs code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import io\n",
                "import contextlib\n",
                "\n",
                "class SimpleREPL:\n",
                "    def __init__(self, variables=None):\n",
                "        self.variables = variables if variables else {}\n",
                "\n",
                "    def execute(self, code):\n",
                "        f = io.StringIO()\n",
                "        with contextlib.redirect_stdout(f):\n",
                "            try:\n",
                "                # The Magic: We run the code with access to 'self.variables'\n",
                "                exec(code, self.variables)\n",
                "            except Exception as e:\n",
                "                print(f\"Error: {e}\")\n",
                "        return f.getvalue()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: LLM Client\n",
                "\n",
                "We need a standard client to talk to the model. We'll use the new Gemini v2 SDK (`google-genai`) for this.\n",
                "\n",
                "You can install it with: `pip install google-genai`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google import genai\n",
                "import os\n",
                "\n",
                "class GeminiClient:\n",
                "    def __init__(self):\n",
                "        # 1. Initialize the new Gemini v2 Client\n",
                "        # The client will automatically use 'GOOGLE_API_KEY' from environment variables if not provided,\n",
                "        # or we can explicitly pass it.\n",
                "        api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
                "        self.client = genai.Client(api_key=api_key)\n",
                "        self.model_name = \"gemini-2.0-flash\"\n",
                "\n",
                "    def completion(self, prompt: str) -> str:\n",
                "        # Standard completion wrapper\n",
                "        try:\n",
                "            response = self.client.models.generate_content(\n",
                "                model=self.model_name,\n",
                "                contents=prompt\n",
                "            )\n",
                "            return response.text\n",
                "        except Exception as e:\n",
                "            return f\"Error: {e}\"\n",
                "\n",
                "    def chat(self, messages: list[dict]) -> str:\n",
                "        # 2. Convert RLM messages to Gemini format\n",
                "        # For simplicity, we concatenate them into a single string here.\n",
                "        full_prompt = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages])\n",
                "        \n",
                "        try:\n",
                "            response = self.client.models.generate_content(\n",
                "                model=self.model_name,\n",
                "                contents=full_prompt\n",
                "            )\n",
                "            # Helper to strip markdown code blocks from the response\n",
                "            return self._clean_code(response.text)\n",
                "        except Exception as e:\n",
                "            return f\"print('Error: {e}')\"\n",
                "\n",
                "    def _clean_code(self, text):\n",
                "        if \"```\" in text:\n",
                "            try:\n",
                "                return text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
                "            except IndexError:\n",
                "                # Fallback if no python tag or different format\n",
                "                return text.split(\"```\")[1].strip()\n",
                "        return text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: The Recursive Agent\n",
                "\n",
                "This is the RLM. Notice how `completion` creates a REPL, adds context, and—crucially—adds `self` (as `rlm`) to the variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleRLM:\n",
                "    def __init__(self, client):\n",
                "        self.client = client\n",
                "\n",
                "    def completion(self, prompt, context=None):\n",
                "        # 1. Define the Proxy Function\n",
                "        # This allows the model to \"call itself\" (or at least call the LLM)\n",
                "        def llm_query(query):\n",
                "            return self.client.completion(query)\n",
                "\n",
                "        # 2. Prepare the Environment\n",
                "        variables = {\n",
                "            \"context\": context,\n",
                "            \"llm_query\": llm_query\n",
                "        }\n",
                "        repl = SimpleREPL(variables)\n",
                "\n",
                "        # 3. The System Prompt\n",
                "        system_msg = (\n",
                "            \"You are a Recursive Language Model. \"\n",
                "            \"You have a variable 'context' (str). \"\n",
                "            \"You have a function 'llm_query(prompt)' to ask the LLM questions. \"\n",
                "            \"Write Python code to process the context. \"\n",
                "            \"If the context is large, split it and call llm_query on chunks. \"\n",
                "            \"Set 'final_answer' variable if you have the result, or print it. \"\n",
                "            \"IMPORTANT: If you define a function, YOU MUST CALL IT at the end.\"\n",
                "        )\n",
                "\n",
                "        # 4. Get the Program\n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": system_msg},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ]\n",
                "        \n",
                "        # (In a real app, you'd loop this: Thought -> Code -> Obs -> Thought)\n",
                "        print(f\"--- RLM Thinking on: {prompt} ---\")\n",
                "        code = self.client.chat(messages) \n",
                "        print(f\"--- Generated Code ---\\n{code}\")\n",
                "\n",
                "        # 5. Execute and Return\n",
                "        result = repl.execute(code)\n",
                "        \n",
                "        # If the code set a 'final_answer' variable, return that.\n",
                "        if \"final_answer\" in repl.variables:\n",
                "            return repl.variables[\"final_answer\"]\n",
                "        return result.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Recursive Trace\n",
                "\n",
                "Let's walk through a real Recursive Trace to see the \"Active Attention\" claim in action.\n",
                "\n",
                "Imagine you have a dense, 100,000-character document—perhaps a legal contract or a technical manual—and a simple prompt: \"Summarize this.\"\n",
                "\n",
                "A standard LLM may attempt to eat the elephant in one bite. It forces all 100,000 characters into its active memory simultaneously.\n",
                "- **The Action**: It attends to every token at once, treating the noise and the signal with equal weight.\n",
                "- **The Failure**: The \"Lost in the Middle\" phenomenon kicks in. The model hallucinates or forgets crucial details buried in paragraph 400 because its attention mechanism is stretched too thin.\n",
                "- **The Cost**: You pay for 100k tokens of compute, much of which is wasted on \"cognitive overload.\"\n",
                "\n",
                "The Recursive Language Model acts less like an overwhelmed reader and more like a Project Manager.\n",
                "\n",
                "### Phase 1: The Assessment (Level 0)\n",
                "The model receives the input but doesn't read it yet. It checks the variable `len(context)` and sees 100,000. It immediately realizes: \"This is above my optimal cognitive load.\"\n",
                "It makes an executive decision: \"I will split this into 5 manageable chunks and hire sub-agents to process them.\"\n",
                "\n",
                "### Phase 2: The Plan (Code Generation)\n",
                "The model writes and executes a Python script to handle the logistics. It dynamically creates a Map-Reduce architecture on the fly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- RLM Thinking on: Summarize this text ---\n",
                        "--- Generated Code ---\n",
                        "context = \"\"\"\n",
                        "The Amazon rainforest is a vast and biodiverse ecosystem located in the Amazon basin of South America. Covering an area of approximately 8 million square kilometers, it spans across nine countries: Brazil, Peru, Colombia, Venezuela, Ecuador, Bolivia, Guyana, Suriname, and French Guiana. The majority of the rainforest is within Brazil.\n",
                        "\n",
                        "The Amazon rainforest is often referred to as the \"lungs of the Earth\" due to its crucial role in producing oxygen and absorbing carbon dioxide. It plays a vital role in regulating the global climate and maintaining the stability of the planet's ecosystems. However, this label is somewhat misleading as the rainforest itself consumes nearly all of the oxygen it produces through respiration. Its real importance lies in its role in carbon sequestration and influencing regional and global weather patterns.\n",
                        "\n",
                        "Biodiversity is exceptionally high in the Amazon rainforest, with an estimated 10% of the world's known species residing within its borders. It is home to millions of insects, plants, birds, and other animals, many of which are still undiscovered. Iconic species such as jaguars, monkeys, macaws, and poison dart frogs inhabit the rainforest.\n",
                        "\n",
                        "The Amazon River, the second longest river in the world, flows through the heart of the rainforest. The river and its tributaries are essential for transportation, providing water for drinking and agriculture, and supporting a diverse array of aquatic life.\n",
                        "\n",
                        "The Amazon rainforest faces numerous threats, including deforestation, logging, mining, and agriculture. Deforestation is particularly concerning, as it leads to habitat loss, soil erosion, and increased carbon emissions. Indigenous communities who call the rainforest home are also threatened by these activities.\n",
                        "\n",
                        "Efforts are underway to protect the Amazon rainforest, including conservation initiatives, sustainable development projects, and government policies aimed at curbing deforestation and promoting responsible land use. However, the scale of the challenge is immense, and continued vigilance and action are needed to ensure the long-term survival of this invaluable ecosystem.\n",
                        "\"\"\"\n",
                        "\n",
                        "def summarize_text(text, max_chunk_size=2000):\n",
                        "    \"\"\"\n",
                        "    Summarizes a large text by splitting it into chunks and querying the LLM.\n",
                        "    \"\"\"\n",
                        "    text_length = len(text)\n",
                        "    if text_length <= max_chunk_size:\n",
                        "        prompt = f\"Summarize the following text:\\n{text}\"\n",
                        "        summary = llm_query(prompt)\n",
                        "        return summary\n",
                        "\n",
                        "    else:\n",
                        "        # Split the text into chunks\n",
                        "        chunks = [text[i:i + max_chunk_size] for i in range(0, text_length, max_chunk_size)]\n",
                        "        chunk_summaries = []\n",
                        "        for i, chunk in enumerate(chunks):\n",
                        "            prompt = f\"Summarize this part of a document. This is chunk {i+1} of {len(chunks)}:\\n{chunk}\"\n",
                        "            chunk_summaries.append(llm_query(prompt))\n",
                        "        # Summarize the summaries\n",
                        "        combined_summary = \"\\n\".join(chunk_summaries)\n",
                        "        final_summary = llm_query(f\"Summarize the following text which contains summaries of parts of the same document:\\n{combined_summary}\")\n",
                        "\n",
                        "        return final_summary\n",
                        "\n",
                        "def llm_query(prompt):\n",
                        "    \"\"\"\n",
                        "    Placeholder for the actual LLM query.\n",
                        "    \"\"\"\n",
                        "    print(\"Running llm_query with prompt:\", prompt)\n",
                        "    if \"Amazon rainforest\" in prompt:\n",
                        "        return \"The Amazon rainforest is a large, biodiverse ecosystem facing threats like deforestation, requiring conservation efforts.\" # dummy answer\n",
                        "\n",
                        "    return \"Summary Placeholder\"\n",
                        "\n",
                        "final_answer = summarize_text(context)\n",
                        "print(final_answer)\n",
                        "The Amazon rainforest is a large, biodiverse ecosystem facing threats like deforestation, requiring conservation efforts.\n"
                    ]
                }
            ],
            "source": [
                "# Example Usage Mockup\n",
                "# Ensure you have your GOOGLE_API_KEY set in your environment variables before running this.\n",
                "\n",
                "client = GeminiClient()\n",
                "rlm = SimpleRLM(client)\n",
                "# Create a dummy large context\n",
                "large_context = \"This is a sentence. \" * 10000 \n",
                "answer = rlm.completion(\"Summarize this text\", context=large_context)\n",
                "print(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 3: The Delegation (Level 1)\n",
                "Five \"Sub-Agents\" wake up. Each one receives exactly 20,000 characters—a perfectly manageable workload.\n",
                "- Agent 1 reads the intro and summarizes it perfectly.\n",
                "- Agent 3 reads the middle section (usually the \"lost\" part) with full clarity because, to Agent 3, this is the only context that exists.\n",
                "\n",
                "They all return their partial summaries to the manager.\n",
                "\n",
                "### Phase 4: The Synthesis\n",
                "Back at Level 0, the Manager receives 5 concise summaries. The massive noise of the original document is gone. It calls itself one last time with the compressed context to generate a flowing, cohesive final answer.\n",
                "\n",
                "By switching to this recursive method, we drastically changed the math of the operation:\n",
                "- Map Step Load: 20k tokens (manageable).\n",
                "- Reduce Step Load: ~2k tokens (pure signal).\n",
                "- Max Cognitive Load: 20k tokens.\n",
                "\n",
                "We processed a 100k document, but the model never had to \"hold\" more than 20% of it at any given time.\n",
                "\n",
                "---\n",
                "The full original source code implementation is available at github.com/alexzhang13/rlm."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
