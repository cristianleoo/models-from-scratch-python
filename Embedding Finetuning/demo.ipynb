{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    InputExample,\n",
    "    losses,\n",
    "    evaluation,\n",
    "    util,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Set up Google Gemini API Key | Replace with your own key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key\" \n",
    "\n",
    "# Setup logging\n",
    "logger.add(\"finetuning_log.log\", rotation=\"10MB\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 10:27:13.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDownloading NVIDIA 10-K report...\u001b[0m\n",
      "Downloading 10-K report: 100%|██████████| 1.02M/1.02M [00:00<00:00, 4.79MB/s]\n",
      "\u001b[32m2025-02-15 10:27:14.481\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdownload_10k\u001b[0m:\u001b[36m17\u001b[0m - \u001b[32m\u001b[1mDownloaded 10-K report: nvidia_10k.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1. Download NVIDIA's latest 10-K report\n",
    "def download_10k(url: str, filename: str) -> None:\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(filename, \"wb\") as f, tqdm(\n",
    "        desc=\"Downloading 10-K report\",\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "    logger.success(f\"Downloaded 10-K report: {filename}\")\n",
    "\n",
    "url = \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf\"\n",
    "filename = \"nvidia_10k.pdf\"\n",
    "\n",
    "logger.info(\"Downloading NVIDIA 10-K report...\")\n",
    "download_10k(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 10:27:14.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mExtracting text from PDF...\u001b[0m\n",
      "Extracting text from PDF: 100%|██████████| 96/96 [00:03<00:00, 25.00page/s]\n",
      "\u001b[32m2025-02-15 10:27:18.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_text_from_pdf\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mExtracted text from PDF (368809 characters).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 2. Extract text from the 10-K PDF\n",
    "def extract_text_from_pdf(filename: str) -> str:\n",
    "    reader = PdfReader(filename)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in tqdm(reader.pages, desc=\"Extracting text from PDF\", unit=\"page\"):\n",
    "        text += page.extract_text() + \"\\n\\n\"  # Keep paragraph separation\n",
    "\n",
    "    logger.info(f\"Extracted text from PDF ({len(text)} characters).\")\n",
    "    return text\n",
    "\n",
    "logger.info(\"Extracting text from PDF...\")\n",
    "text = extract_text_from_pdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 10:27:18.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mSegmenting report into paragraphs...\u001b[0m\n",
      "Processing paragraphs: 100%|██████████| 97/97 [00:00<00:00, 11805.68chunk/s]\n",
      "\u001b[32m2025-02-15 10:27:18.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mparagraph_chunking\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mSegmented text into 96 paragraph-based sections.\u001b[0m\n",
      "\u001b[32m2025-02-15 10:27:18.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mGenerated 96 paragraph-based sections.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 3. Paragraph-Based Chunking\n",
    "def paragraph_chunking(text: str, min_words: int = 50, max_words: int = 300) -> List[str]:\n",
    "    \"\"\"Process paragraphs with progress bar\"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in tqdm(paragraphs, desc=\"Processing paragraphs\", unit=\"chunk\"):\n",
    "        words = para.split()\n",
    "        if len(words) < min_words:\n",
    "            continue\n",
    "\n",
    "        if len(current_chunk.split()) + len(words) <= max_words:\n",
    "            current_chunk += \" \" + para\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    logger.info(f\"Segmented text into {len(chunks)} paragraph-based sections.\")\n",
    "    return chunks\n",
    "\n",
    "logger.info(\"Segmenting report into paragraphs...\")\n",
    "paragraphs = paragraph_chunking(text)\n",
    "\n",
    "logger.info(f\"Generated {len(paragraphs)} paragraph-based sections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate Questions Using Gemini via LangChain\n",
    "def generate_questions_gemini(section_text: str) -> List[str]:\n",
    "    rate_limiter = InMemoryRateLimiter(requests_per_second=0.1)\n",
    "    # llm = GoogleGenerativeAI(\n",
    "    #     model=\"gemini-2.0-flash\",\n",
    "    #     api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    #     rate_limiter=rate_limiter\n",
    "    # )\n",
    "    llm = OllamaLLM(\n",
    "        model=\"deepseek-r1:latest\",\n",
    "        host=\"http://localhost:11434\",\n",
    "    )\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"Generate one question based on this financial report section:\\n\\n{context}\",\n",
    "        input_variables=[\"context\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    try:\n",
    "        response = chain.invoke({\"context\": section_text})  # Limit text length\n",
    "        return response  # Returns list of generated questions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating questions: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 10:39:17.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mGenerating question-context pairs using Gemini...\u001b[0m\n",
      "Generating Q&A pairs:   0%|          | 0/96 [00:00<?, ?pair/s]\u001b[32m2025-02-15 10:39:17.096\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:17.179\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:   2%|▏         | 2/96 [00:00<00:07, 12.04pair/s]\u001b[32m2025-02-15 10:39:17.261\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:17.344\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:   4%|▍         | 4/96 [00:00<00:07, 12.06pair/s]\u001b[32m2025-02-15 10:39:17.429\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:17.511\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:   6%|▋         | 6/96 [00:00<00:07, 12.05pair/s]\u001b[32m2025-02-15 10:39:17.593\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:17.675\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:   8%|▊         | 8/96 [00:00<00:07, 12.08pair/s]\u001b[32m2025-02-15 10:39:17.758\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:17.839\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  10%|█         | 10/96 [00:00<00:07, 12.14pair/s]\u001b[32m2025-02-15 10:39:17.921\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.003\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  12%|█▎        | 12/96 [00:00<00:06, 12.16pair/s]\u001b[32m2025-02-15 10:39:18.107\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.189\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  15%|█▍        | 14/96 [00:01<00:07, 11.65pair/s]\u001b[32m2025-02-15 10:39:18.271\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.353\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  17%|█▋        | 16/96 [00:01<00:06, 11.81pair/s]\u001b[32m2025-02-15 10:39:18.436\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.517\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  19%|█▉        | 18/96 [00:01<00:06, 11.93pair/s]\u001b[32m2025-02-15 10:39:18.600\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.681\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  21%|██        | 20/96 [00:01<00:06, 12.03pair/s]\u001b[32m2025-02-15 10:39:18.763\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "\u001b[32m2025-02-15 10:39:18.845\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_gemini\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mError generating questions: model 'deepseek-r1:latest' not found (status code: 404)\u001b[0m\n",
      "Generating Q&A pairs:  23%|██▎       | 22/96 [00:01<00:06, 11.52pair/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pairs\n\u001b[1;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating question-context pairs using Gemini...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_question_context_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraphs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Q&A pairs for fine-tuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mgenerate_question_context_pairs\u001b[0;34m(paragraphs)\u001b[0m\n\u001b[1;32m      3\u001b[0m pairs: List[InputExample] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m tqdm(paragraphs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating Q&A pairs\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m      7\u001b[0m         pairs\u001b[38;5;241m.\u001b[39mappend(InputExample(texts\u001b[38;5;241m=\u001b[39m[question, para]))\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36mgenerate_questions_gemini\u001b[0;34m(section_text)\u001b[0m\n\u001b[1;32m      3\u001b[0m rate_limiter \u001b[38;5;241m=\u001b[39m InMemoryRateLimiter(requests_per_second\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# llm = GoogleGenerativeAI(\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     model=\"gemini-2.0-flash\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     api_key=os.getenv(\"GOOGLE_API_KEY\"),\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     rate_limiter=rate_limiter\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOllamaLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-r1:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:11434\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     15\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate one question based on this financial report section:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m|\u001b[39m llm\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/langchain_ollama/llms.py:191\u001b[0m, in \u001b[0;36mOllamaLLM._set_clients\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m client_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m Client(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_kwargs)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client \u001b[38;5;241m=\u001b[39m \u001b[43mAsyncClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/ollama/_client.py:618\u001b[0m, in \u001b[0;36mAsyncClient.__init__\u001b[0;34m(self, host, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, host: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhttpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAsyncClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/ollama/_client.py:91\u001b[0m, in \u001b[0;36mBaseClient.__init__\u001b[0;34m(self, client, host, follow_redirects, timeout, headers, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     75\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     76\u001b[0m   client,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m  Creates a httpx client. Default parameters are the same as those defined in httpx\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m  except for the following:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m  `kwargs` are passed to the httpx client.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parse_host\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOLLAMA_HOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Lowercase all headers to ensure override\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m      \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccept\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mollama-python/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m__version__\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmachine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) Python/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/httpx/_client.py:1402\u001b[0m, in \u001b[0;36mAsyncClient.__init__\u001b[0;34m(self, auth, params, headers, cookies, verify, cert, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, trust_env, default_encoding)\u001b[0m\n\u001b[1;32m   1399\u001b[0m allow_env_proxies \u001b[38;5;241m=\u001b[39m trust_env \u001b[38;5;129;01mand\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m proxy_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_proxy_map(proxy, allow_env_proxies)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, AsyncBaseTransport \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1413\u001b[0m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1425\u001b[0m }\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/httpx/_client.py:1445\u001b[0m, in \u001b[0;36mAsyncClient._init_transport\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transport\n\u001b[0;32m-> 1445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAsyncHTTPTransport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/httpx/_transports/default.py:297\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.__init__\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[1;32m    296\u001b[0m proxy \u001b[38;5;241m=\u001b[39m Proxy(url\u001b[38;5;241m=\u001b[39mproxy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxy, (\u001b[38;5;28mstr\u001b[39m, URL)) \u001b[38;5;28;01melse\u001b[39;00m proxy\n\u001b[0;32m--> 297\u001b[0m ssl_context \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mAsyncConnectionPool(\n\u001b[1;32m    301\u001b[0m         ssl_context\u001b[38;5;241m=\u001b[39mssl_context,\n\u001b[1;32m    302\u001b[0m         max_connections\u001b[38;5;241m=\u001b[39mlimits\u001b[38;5;241m.\u001b[39mmax_connections,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39msocket_options,\n\u001b[1;32m    311\u001b[0m     )\n",
      "File \u001b[0;32m~/workplace/models-from-scratch-python/venv/lib/python3.11/site-packages/httpx/_config.py:35\u001b[0m, in \u001b[0;36mcreate_ssl_context\u001b[0;34m(verify, cert, trust_env)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_env \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSL_CERT_FILE\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m \u001b[43mssl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_default_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSL_CERT_FILE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m trust_env \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSL_CERT_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39mcreate_default_context(capath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSL_CERT_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.11/ssl.py:770\u001b[0m, in \u001b[0;36mcreate_default_context\u001b[0;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(purpose)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[0;32m--> 770\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mverify_mode \u001b[38;5;241m!=\u001b[39m CERT_NONE:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs(purpose)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. Create Q&A Pairs\n",
    "def generate_question_context_pairs(paragraphs: List[str]) -> List[InputExample]:\n",
    "    pairs: List[InputExample] = []\n",
    "    for para in tqdm(paragraphs, desc=\"Generating Q&A pairs\", unit=\"pair\"):\n",
    "        questions = generate_questions_gemini(para)\n",
    "        for question in questions:\n",
    "            pairs.append(InputExample(texts=[question, para]))\n",
    "\n",
    "    logger.info(f\"Generated {len(pairs)} Q&A pairs for fine-tuning.\")\n",
    "    return pairs\n",
    "\n",
    "logger.info(\"Generating question-context pairs using Gemini...\")\n",
    "pairs = generate_question_context_pairs(paragraphs)\n",
    "logger.info(f\"Generated {len(pairs)} Q&A pairs for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q&A pairs to a pickle file\n",
    "with open(\"qa_pairs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Q&A Pairs\n",
    "with open(\"qa_pairs.pkl\", \"rb\") as f:\n",
    "    pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluation: Compare Pre and Post Fine-Tuning Performance\n",
    "def evaluate_model(model, questions, contexts):\n",
    "    \"\"\"\n",
    "    Evaluates the model by computing cosine similarity between questions and their corresponding contexts.\n",
    "\n",
    "    Args:\n",
    "        model: The sentence embedding model.\n",
    "        questions (List[str]): List of financial questions.\n",
    "        contexts (List[str]): List of corresponding report contexts.\n",
    "\n",
    "    Returns:\n",
    "        float: Average cosine similarity score.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for question, context in tqdm(zip(questions, contexts), total=len(questions), desc=\"Evaluating Model\", unit=\"pair\"):\n",
    "        question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "        context_embedding = model.encode(context, convert_to_tensor=True)\n",
    "        similarity_score = util.pytorch_cos_sim(question_embedding, context_embedding).item()\n",
    "        scores.append(similarity_score)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    logger.info(f\"Evaluation Complete - Average Cosine Similarity: {avg_score:.4f}\")\n",
    "    \n",
    "    return avg_score\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pre-training evaluation\n",
    "pre_score = evaluate_model(model, [q.texts[0] for q in pairs], [q.texts[1] for q in pairs])\n",
    "logger.info(f\"Pre-Fine-Tuning Cosine Similarity: {pre_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/5200 : < :, Epoch 0.01/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# 6. Fine-Tune Using Sentence-Transformer's `fit` method\n",
    "def fine_tune_model(model: SentenceTransformer, pairs, sample_ratio=0.1, epochs=50, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Fine-tunes using DataLoader with SubsetRandomSampler for efficient sampling.\n",
    "    \"\"\"\n",
    "    # Create indices and sample subset\n",
    "    # indices = np.arange(len(pairs))\n",
    "    # sample_size = int(len(pairs) * sample_ratio)\n",
    "    # sampler = SubsetRandomSampler(np.random.choice(indices, sample_size, replace=False))\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        pairs, \n",
    "        batch_size=batch_size,\n",
    "        # sampler=sampler,  # Use sampler instead of shuffle\n",
    "    )\n",
    "\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        optimizer_params={\n",
    "            \"lr\": learning_rate,\n",
    "            \"eps\": 1e-6,\n",
    "        },\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.success(\"Fine-tuning complete!\")\n",
    "    \n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# model = SentenceTransformer(\"finetuned_model\")\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_model(model, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save(\"finetuned_model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 1657/1657 [06:07<00:00,  4.50pair/s]\n",
      "\u001b[32m2025-02-15 10:12:22.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEvaluation Complete - Average Cosine Similarity: 0.2494\u001b[0m\n",
      "\u001b[32m2025-02-15 10:12:22.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mPost-Fine-Tuning Cosine Similarity: 0.2494\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Post-training evaluation\n",
    "model = SentenceTransformer(\"finetuned_model\")\n",
    "post_score = evaluate_model(model, [q.texts[0] for q in pairs], [q.texts[1] for q in pairs])\n",
    "logger.info(f\"Post-Fine-Tuning Cosine Similarity: {post_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
