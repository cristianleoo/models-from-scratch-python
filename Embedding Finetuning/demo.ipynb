{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, util\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set up Google Gemini API Key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key\"\n",
    "\n",
    "# Setup logging\n",
    "logger.add(\"finetuning_log.log\", rotation=\"10MB\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-12 19:52:30.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mStarting NVIDIA 10-K processing pipeline...\u001b[0m\n",
      "\u001b[32m2025-02-12 19:52:30.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mDownloading NVIDIA 10-K report...\u001b[0m\n",
      "Downloading 10-K report: 100%|██████████| 1.02M/1.02M [00:00<00:00, 7.02MB/s]\n",
      "\u001b[32m2025-02-12 19:52:30.431\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdownload_10k\u001b[0m:\u001b[36m23\u001b[0m - \u001b[32m\u001b[1mDownloaded 10-K report: nvidia_10k.pdf\u001b[0m\n",
      "\u001b[32m2025-02-12 19:52:30.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mExtracting text from PDF...\u001b[0m\n",
      "Extracting text from PDF: 100%|██████████| 96/96 [00:03<00:00, 29.64page/s]\n",
      "\u001b[32m2025-02-12 19:52:33.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_text_from_pdf\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mExtracted text from PDF (368809 characters).\u001b[0m\n",
      "\u001b[32m2025-02-12 19:52:33.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mSegmenting report into paragraphs...\u001b[0m\n",
      "Processing paragraphs: 100%|██████████| 97/97 [00:00<00:00, 16006.27chunk/s]\n",
      "\u001b[32m2025-02-12 19:52:33.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mparagraph_chunking\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mSegmented text into 96 paragraph-based sections.\u001b[0m\n",
      "\u001b[32m2025-02-12 19:52:33.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mGenerated 96 paragraph-based sections.\u001b[0m\n",
      "\u001b[32m2025-02-12 19:52:33.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mGenerating question-context pairs using Gemini...\u001b[0m\n",
      "Generating Q&A pairs:  29%|██▉       | 28/96 [01:29<03:40,  3.25s/pair]Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Generating Q&A pairs:  44%|████▍     | 42/96 [02:18<03:10,  3.53s/pair]Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Generating Q&A pairs:  65%|██████▍   | 62/96 [03:26<01:48,  3.19s/pair]Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Generating Q&A pairs:  81%|████████▏ | 78/96 [04:26<00:58,  3.28s/pair]Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Generating Q&A pairs: 100%|██████████| 96/96 [05:33<00:00,  3.47s/pair]\n",
      "\u001b[32m2025-02-12 19:58:07.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_question_context_pairs\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mGenerated 1667 Q&A pairs for fine-tuning.\u001b[0m\n",
      "\u001b[32m2025-02-12 19:58:07.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1mGenerated 1667 Q&A pairs for fine-tuning.\u001b[0m\n",
      "\u001b[32m2025-02-12 19:58:07.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mStarting fine-tuning...\u001b[0m\n",
      "Training Epoch 1/10: 100%|██████████| 53/53 [00:16<00:00,  3.22batch/s, loss=1.05]\n",
      "\u001b[32m2025-02-12 19:58:26.967\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 1/10 - Average Loss: 3.3171\u001b[0m\n",
      "Training Epoch 2/10: 100%|██████████| 53/53 [00:10<00:00,  5.15batch/s, loss=0.981]\n",
      "\u001b[32m2025-02-12 19:58:37.256\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 2/10 - Average Loss: 3.3162\u001b[0m\n",
      "Training Epoch 3/10: 100%|██████████| 53/53 [00:09<00:00,  5.59batch/s, loss=1]   \n",
      "\u001b[32m2025-02-12 19:58:46.741\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 3/10 - Average Loss: 3.3170\u001b[0m\n",
      "Training Epoch 4/10: 100%|██████████| 53/53 [00:09<00:00,  5.56batch/s, loss=1.08]\n",
      "\u001b[32m2025-02-12 19:58:56.284\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 4/10 - Average Loss: 3.3175\u001b[0m\n",
      "Training Epoch 5/10: 100%|██████████| 53/53 [00:09<00:00,  5.70batch/s, loss=0.963]\n",
      "\u001b[32m2025-02-12 19:59:05.589\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 5/10 - Average Loss: 3.3163\u001b[0m\n",
      "Training Epoch 6/10: 100%|██████████| 53/53 [00:09<00:00,  5.73batch/s, loss=1.01]\n",
      "\u001b[32m2025-02-12 19:59:14.838\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 6/10 - Average Loss: 3.3173\u001b[0m\n",
      "Training Epoch 7/10: 100%|██████████| 53/53 [00:09<00:00,  5.58batch/s, loss=1.03]\n",
      "\u001b[32m2025-02-12 19:59:24.332\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 7/10 - Average Loss: 3.3169\u001b[0m\n",
      "Training Epoch 8/10: 100%|██████████| 53/53 [00:09<00:00,  5.59batch/s, loss=1.01]\n",
      "\u001b[32m2025-02-12 19:59:33.808\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 8/10 - Average Loss: 3.3165\u001b[0m\n",
      "Training Epoch 9/10: 100%|██████████| 53/53 [00:09<00:00,  5.72batch/s, loss=0.973]\n",
      "\u001b[32m2025-02-12 19:59:43.073\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 9/10 - Average Loss: 3.3159\u001b[0m\n",
      "Training Epoch 10/10: 100%|██████████| 53/53 [00:09<00:00,  5.71batch/s, loss=0.94]\n",
      "\u001b[32m2025-02-12 19:59:52.349\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[32m\u001b[1mEpoch 10/10 - Average Loss: 3.3153\u001b[0m\n",
      "\u001b[32m2025-02-12 19:59:52.350\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_embedding_model\u001b[0m:\u001b[36m143\u001b[0m - \u001b[32m\u001b[1mFine-tuning complete!\u001b[0m\n",
      "\u001b[32m2025-02-12 19:59:52.351\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m171\u001b[0m - \u001b[32m\u001b[1mPipeline execution complete!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1. Download NVIDIA's latest 10-K report\n",
    "def download_10k(url: str, filename: str) -> None:\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(filename, \"wb\") as f, tqdm(\n",
    "        desc=\"Downloading 10-K report\",\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "    logger.success(f\"Downloaded 10-K report: {filename}\")\n",
    "\n",
    "url = \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf\"\n",
    "filename = \"nvidia_10k.pdf\"\n",
    "\n",
    "logger.info(\"Downloading NVIDIA 10-K report...\")\n",
    "download_10k(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract text from the 10-K PDF\n",
    "def extract_text_from_pdf(filename: str) -> str:\n",
    "    reader = PdfReader(filename)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in tqdm(reader.pages, desc=\"Extracting text from PDF\", unit=\"page\"):\n",
    "        text += page.extract_text() + \"\\n\\n\"  # Keep paragraph separation\n",
    "\n",
    "    logger.info(f\"Extracted text from PDF ({len(text)} characters).\")\n",
    "    return text\n",
    "\n",
    "logger.info(\"Extracting text from PDF...\")\n",
    "text = extract_text_from_pdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Paragraph-Based Chunking\n",
    "def paragraph_chunking(text: str, min_words: int = 50, max_words: int = 300) -> List[str]:\n",
    "    \"\"\"Process paragraphs with progress bar\"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in tqdm(paragraphs, desc=\"Processing paragraphs\", unit=\"chunk\"):\n",
    "        words = para.split()\n",
    "        if len(words) < min_words:\n",
    "            continue\n",
    "\n",
    "        if len(current_chunk.split()) + len(words) <= max_words:\n",
    "            current_chunk += \" \" + para\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    logger.info(f\"Segmented text into {len(chunks)} paragraph-based sections.\")\n",
    "    return chunks\n",
    "\n",
    "logger.info(\"Segmenting report into paragraphs...\")\n",
    "paragraphs = paragraph_chunking(text)\n",
    "\n",
    "logger.info(f\"Generated {len(paragraphs)} paragraph-based sections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate Questions Using Gemini via LangChain\n",
    "def generate_questions_gemini(section_text: str) -> List[str]:\n",
    "    rate_limiter = InMemoryRateLimiter(requests_per_second=0.25)\n",
    "    llm = GoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        rate_limiter=rate_limiter\n",
    "    )\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"Generate five financial questions based on this financial report section:\\n\\n{context}\",\n",
    "        input_variables=[\"context\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | CommaSeparatedListOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({\"context\": section_text})  # Limit text length\n",
    "        return response  # Returns list of generated questions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating questions: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create Q&A Pairs\n",
    "def generate_question_context_pairs(paragraphs: List[str]) -> List[str]:\n",
    "    pairs: List[str] = []\n",
    "    for para in tqdm(paragraphs, desc=\"Generating Q&A pairs\", unit=\"pair\"):\n",
    "        questions = generate_questions_gemini(para)\n",
    "        for question in questions:\n",
    "            pairs.append((question, para))\n",
    "\n",
    "    logger.info(f\"Generated {len(pairs)} Q&A pairs for fine-tuning.\")\n",
    "    return pairs\n",
    "\n",
    "logger.info(\"Generating question-context pairs using Gemini...\")\n",
    "pairs = generate_question_context_pairs(paragraphs)\n",
    "logger.info(f\"Generated {len(pairs)} Q&A pairs for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 1667/1667 [01:19<00:00, 20.85pair/s]\n",
      "\u001b[32m2025-02-12 20:19:36.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEvaluation Complete - Average Cosine Similarity: 0.3449\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation: Compare Pre and Post Fine-Tuning Performance\n",
    "def evaluate_model(model, questions, contexts):\n",
    "    \"\"\"\n",
    "    Evaluates the model by computing cosine similarity between questions and their corresponding contexts.\n",
    "\n",
    "    Args:\n",
    "        model: The sentence embedding model.\n",
    "        questions (List[str]): List of financial questions.\n",
    "        contexts (List[str]): List of corresponding report contexts.\n",
    "\n",
    "    Returns:\n",
    "        float: Average cosine similarity score.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for question, context in tqdm(zip(questions, contexts), total=len(questions), desc=\"Evaluating Model\", unit=\"pair\"):\n",
    "        question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "        context_embedding = model.encode(context, convert_to_tensor=True)\n",
    "        similarity_score = util.pytorch_cos_sim(question_embedding, context_embedding).item()\n",
    "        scores.append(similarity_score)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    logger.info(f\"Evaluation Complete - Average Cosine Similarity: {avg_score:.4f}\")\n",
    "    \n",
    "    return avg_score\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pre-training evaluation\n",
    "pre_score = evaluate_model(model, [q.texts[0] for q in pairs], [q.texts[1] for q in pairs])\n",
    "logger.info(f\"Pre-Fine-Tuning Cosine Similarity: {pre_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 1667/1667 [01:19<00:00, 20.88pair/s]\n",
      "\u001b[32m2025-02-12 21:28:57.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEvaluation Complete - Average Cosine Similarity: 0.3449\u001b[0m\n",
      "\u001b[32m2025-02-12 21:28:57.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mPost-Fine-Tuning Cosine Similarity: 0.3449\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 6. Fine-Tune Using Sentence-Transformer's `fit`\n",
    "def fine_tune_model(model, pairs, epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Fine-tunes the Sentence-Transformer model with MNR loss.\n",
    "    \"\"\"\n",
    "    train_dataloader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=1000,\n",
    "        optimizer_params={\"lr\": learning_rate},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.success(\"Fine-tuning complete!\")\n",
    "    \n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(\"finetuned_model\")\n",
    "\n",
    "# Post-training evaluation\n",
    "post_score = evaluate_model(model, [q for q, _ in pairs], [c for _, c in pairs])\n",
    "logger.info(f\"Post-Fine-Tuning Cosine Similarity: {post_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training evaluation\n",
    "post_score = evaluate_model(model, [q.texts[0] for q in pairs], [q.texts[1] for q in pairs])\n",
    "logger.info(f\"Post-Fine-Tuning Cosine Similarity: {post_score:.4f}\")\n",
    "\n",
    "logger.success(\"Pipeline execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
