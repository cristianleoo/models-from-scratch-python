{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GANDALF: Gated Adaptive Network for Deep Automated Learning of Features**\n",
    "\n",
    "*By Cristian Leo*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# pytorch_tabular for GANDALF\n",
    "from pytorch_tabular.tabular_model import TabularModel\n",
    "from pytorch_tabular.models.gandalf import GANDALFConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "# pytorch for MLP\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GANDALF components from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "#### Softmax ####\n",
      "[[0.09003 0.24473 0.66524]\n",
      " [0.09003 0.24473 0.66524]]\n",
      "\n",
      "#### Temperature-scaled softmax ####\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of x.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy array of shape (N, D), where N is the number of samples and D is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - softmax_output: numpy array of shape (N, D), the softmax probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Ensure numerical stability by subtracting the max value from each row\n",
    "    x_max = np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    softmax_output = exp_x / sum_exp_x\n",
    "    return softmax_output.round(5) # rounding for better readability\n",
    "\n",
    "def t_softmax(input, t=None, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute the temperature-scaled softmax of input.\n",
    "\n",
    "    Parameters:\n",
    "    - input: numpy array of shape (N, D), where N is the number of samples and D is the number of classes.\n",
    "    - t: float, temperature parameter. If None, the default value of 0.5 is used. Higher values of t result in a softer probability distribution.\n",
    "    - dim: int, the dimension along which the softmax is computed.\n",
    "\n",
    "    Returns:\n",
    "    - softmax_output: numpy array of shape (N, D), the softmax probabilities for each class.\n",
    "    \"\"\"\n",
    "    if t is None:\n",
    "        t = 0.5\n",
    "    assert np.all(t >= 0.0)\n",
    "    maxes = np.max(input, axis=dim, keepdims=True)\n",
    "    input_minus_maxes = input - maxes\n",
    "\n",
    "    w = np.maximum(input_minus_maxes + t, 0) + 1e-8 # this is the ReLU function with a small epsilon\n",
    "    e_x = np.exp(input_minus_maxes + np.log(w))\n",
    "    return (e_x / np.sum(e_x, axis=dim, keepdims=True)).round(5) # rounding for better readability\n",
    "\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\n#### Softmax ####\")\n",
    "print(softmax(x))\n",
    "print(\"\\n#### Temperature-scaled softmax ####\")\n",
    "print(t_softmax(x, t=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "\n",
      "Output:\n",
      "[[ 0.98712015  0.63038223  0.0652355   0.70431647]\n",
      " [ 4.34546121  0.06498121 -0.18498697 -0.98712291]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dropout(x, rate=0.0):\n",
    "    \"\"\"\n",
    "    Apply dropout to the input array x.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy array.\n",
    "    - rate: float, dropout rate. Probability of setting a value to zero.\n",
    "\n",
    "    Returns:\n",
    "    - x: numpy array, output tensor after applying dropout\n",
    "    \"\"\"\n",
    "    if rate > 0.0:\n",
    "        keep_prob = 1 - rate\n",
    "        mask = np.random.rand(*x.shape) < keep_prob\n",
    "        return np.where(mask, x / keep_prob, 0)\n",
    "    return x\n",
    "\n",
    "class GatedFeatureLearningUnit:\n",
    "    def __init__(self, n_features_in, n_stages, feature_sparsity=0.3, dropout=0.0):\n",
    "        self.n_features_in = n_features_in\n",
    "        self.n_stages = n_stages\n",
    "        self.feature_sparsity = feature_sparsity\n",
    "        self.dropout_rate = dropout\n",
    "        self._build_network()\n",
    "\n",
    "    def _create_feature_mask(self):\n",
    "        feature_masks = np.concatenate([\n",
    "            np.random.beta(a=random.uniform(0.5, 10.0), b=random.uniform(0.5, 10.0), size=(self.n_features_in,))\n",
    "            for _ in range(self.n_stages)\n",
    "        ]).reshape(self.n_stages, self.n_features_in)\n",
    "        return feature_masks\n",
    "\n",
    "    def _build_network(self):\n",
    "        self.W_in = [np.random.randn(2 * self.n_features_in, 2 * self.n_features_in) for _ in range(self.n_stages)]\n",
    "        self.b_in = [np.random.randn(2 * self.n_features_in) for _ in range(self.n_stages)]\n",
    "        self.W_out = [np.random.randn(2 * self.n_features_in, self.n_features_in) for _ in range(self.n_stages)]\n",
    "        self.b_out = [np.random.randn(self.n_features_in) for _ in range(self.n_stages)]\n",
    "        self.feature_masks = self._create_feature_mask()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for d in range(self.n_stages):\n",
    "            feature = t_softmax(self.feature_masks[d], t=self.feature_sparsity) * x\n",
    "\n",
    "            # Gated feature learning unit\n",
    "            h_in = np.dot(np.concatenate([feature, h], axis=-1), self.W_in[d]) + self.b_in[d]\n",
    "            z = sigmoid(h_in[:, :self.n_features_in])\n",
    "            r = sigmoid(h_in[:, self.n_features_in:])\n",
    "            h_out = tanh(np.dot(np.concatenate([r * h, x], axis=-1), self.W_out[d]) + self.b_out[d])\n",
    "            h = dropout((1 - z) * h + z * h_out, self.dropout_rate)\n",
    "        return h\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    \n",
    "glfu = GatedFeatureLearningUnit(n_features_in=4, n_stages=2, feature_sparsity=0.3, dropout=0.0)\n",
    "x = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nOutput:\")\n",
    "print(glfu(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "#### RSoftmax ####\n",
      "[[0.1729912  0.30150792 0.52550087]\n",
      " [0.1729912  0.30150792 0.52550087]]\n"
     ]
    }
   ],
   "source": [
    "class RSoftmax:\n",
    "    \"\"\"\n",
    "    RSoftmax activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: int, the dimension along which the softmax is computed.\n",
    "    - eps: float, small value to avoid division by zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = -1, eps: float = 1e-8):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, t=1.0, axis=-1):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp((x - np.max(x, axis=axis, keepdims=True)) / t)\n",
    "        return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "    @classmethod\n",
    "    def calculate_t(cls, input, r, dim: int = -1, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Calculate the temperature parameter t for the RSoftmax function.\n",
    "\n",
    "        Parameters:\n",
    "        - input: numpy array of shape (N, D), where N is the number of samples and D is the number of classes.\n",
    "        - r: float, the fraction of zeros in the output.\n",
    "        - dim: int, the dimension along which the softmax is computed.\n",
    "        - eps: float, small value to avoid division by zero.\n",
    "\n",
    "        Returns:\n",
    "        - t: numpy array of shape (N, 1), the temperature parameter for each sample.\n",
    "        \"\"\"\n",
    "        assert np.all((0.0 <= r) & (r <= 1.0))\n",
    "\n",
    "        maxes = np.max(input, axis=dim, keepdims=True)\n",
    "        input_minus_maxes = input - maxes\n",
    "\n",
    "        zeros_mask = np.exp(input_minus_maxes) == 0.0\n",
    "        zeros_frac = zeros_mask.sum(axis=dim, keepdims=True).astype(float) / input.shape[dim]\n",
    "\n",
    "        q = np.clip((r - zeros_frac) / (1 - zeros_frac), 0.0, 1.0)\n",
    "        x_minus_maxes = input_minus_maxes * (~zeros_mask).astype(float)\n",
    "        if q.ndim > 1:\n",
    "            t = -np.quantile(x_minus_maxes, q.ravel(), axis=dim, keepdims=True) + eps\n",
    "            t = np.squeeze(t, axis=dim).diagonal().reshape(-1, 1) + eps\n",
    "        else:\n",
    "            t = -np.quantile(x_minus_maxes, q, axis=dim) + eps\n",
    "        return t\n",
    "\n",
    "    def forward(self, input, r):\n",
    "        t = self.calculate_t(input, r, self.dim, self.eps)\n",
    "        return self.softmax(input, t, self.dim)\n",
    "    \n",
    "r_softmax = RSoftmax()\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\n#### RSoftmax ####\")\n",
    "print(r_softmax.forward(x, r=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Application of GANDALF using pytorch_tabular**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = {\n",
    "    'features': 'num', \n",
    "    'target': 'num'\n",
    "}\n",
    "\n",
    "# Select which TASK. Can be either a single TASK or a list of TASK\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1024\n",
    "MAX_EPOCHS = 100\n",
    "EARLY_STOPPING = True\n",
    "OPTIMIZER = \"AdamW\"\n",
    "WEIGHT_DECAY = 1e-6\n",
    "LR_SCHEDULER = \"CosineAnnealingWarmRestarts\"\n",
    "LR_SCHEDULER_PARAMS = {\"T_0\": 10, \"T_mult\": 1, \"eta_min\": 1e-5}\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"gflu_stages\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type = task_type['features']\n",
    "target_type = task_type['target']\n",
    "\n",
    "X = pd.read_parquet('data/data_train.parquet')\n",
    "y = pd.read_parquet(f'data/target_train.parquet')\n",
    "\n",
    "split_ratio = 0.8\n",
    "split = int(len(X)*split_ratio)\n",
    "\n",
    "X_test = X.iloc[split:]\n",
    "y_test = y.iloc[split:]\n",
    "\n",
    "X = X.iloc[:split]\n",
    "y = y.iloc[:split]\n",
    "\n",
    "names = json.load(open('data/attribute_names.json'))\n",
    "cat_indicator = json.load(open('data/categorical_indicator.json'))\n",
    "\n",
    "# Find categorical and numerical features\n",
    "cat_cols = [n for n, c in zip(names, cat_indicator) if c]\n",
    "num_cols = list(set(names)-set(cat_cols))\n",
    "\n",
    "# Assigning classification or regression as task based on target type\n",
    "task_pt = \"classification\" #if target_type!=\"num\" else \"regression\"\n",
    "\n",
    "# PyTorch Tabular expects s single dataframe as input\n",
    "X['target'] = y.values\n",
    "\n",
    "# Cat Cols as Categorical dtype messes with the categorical encoding\n",
    "X[cat_cols] = X[cat_cols].astype(str)\n",
    "X_test[cat_cols] = X_test[cat_cols].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">397</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">140</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m397\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m140\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">410</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">524</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m410\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m524\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">413</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:499</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m413\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:499\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">428</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">574</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: GANDALFModel           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m428\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m574\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: GANDALFModel           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">441</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m441\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m340\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">19:13:59</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">477</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">652</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2024\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m19:13:59\u001b[0m,\u001b[1;36m477\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m652\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /Users/cristianleo/Documents/Documents - Cristian’s Laptop/GitHub/models-from-scratch-python/GANDALF/saved_models exists and is not empty.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    }
   ],
   "source": [
    "# Define Configs | Check API for other options\n",
    "data_config = DataConfig(\n",
    "    target=[\n",
    "        \"target\"\n",
    "    ],\n",
    "    continuous_cols=num_cols,\n",
    "    categorical_cols=cat_cols,\n",
    "    num_workers=3\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    early_stopping=\"valid_loss\" if EARLY_STOPPING else None,  # Monitor valid_loss for early stopping\n",
    "    early_stopping_mode=\"min\",  # Set the mode as min because for val_loss, lower is better\n",
    "    early_stopping_patience=5,  # No. of epochs of degradation training will wait before terminating\n",
    "    checkpoints=\"valid_loss\",  # Save best checkpoint monitoring val_loss\n",
    "    load_best=True,  # After training, load the best checkpoint\n",
    "    progress_bar=\"none\",  # Turning off Progress bar\n",
    "    trainer_kwargs=dict(enable_model_summary=False),  # Turning off model summary\n",
    "#             fast_dev_run=True\n",
    ")\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=OPTIMIZER,\n",
    "    optimizer_params={\"weight_decay\": WEIGHT_DECAY},\n",
    "    lr_scheduler=LR_SCHEDULER,\n",
    "    lr_scheduler_params=LR_SCHEDULER_PARAMS,\n",
    ")\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    dropout=0.2,\n",
    "    initialization=(  # No additional layer in head, just a mapping layer to output_dim\n",
    "        \"kaiming\"\n",
    "    ),\n",
    ").__dict__  # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)\n",
    "model_config = GANDALFConfig(\n",
    "    task=task_pt,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    head=\"LinearHead\",  # Linear Head\n",
    "    head_config=head_config,  # Linear Head Config\n",
    "    **MODEL_PARAMS\n",
    ")\n",
    "# Initialize the Tabular Model\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    verbose=True,\n",
    "    suppress_lightning_logger=True\n",
    ")\n",
    "# If you have separate VAL defined, you can pass in that. If not, PyTorch Tabular will automatically take a sample out of Training to use as Validation\n",
    "tabular_model.fit(\n",
    "    train=X,\n",
    "    seed=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Predict on Test Data\n",
    "prediction = tabular_model.predict(X_test, progress_bar=None)\n",
    "preds = prediction.iloc[:,1].values\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "preds.index = X_test.index\n",
    "acc = (preds.values.round() == y_test.values).mean()\n",
    "print(f\"Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Application of MLP using torch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.2374\n",
      "Epoch 2, Validation Loss: 0.2228\n",
      "Epoch 3, Validation Loss: 0.2157\n",
      "Epoch 4, Validation Loss: 0.2105\n",
      "Epoch 5, Validation Loss: 0.2065\n",
      "Epoch 6, Validation Loss: 0.2020\n",
      "Epoch 7, Validation Loss: 0.1993\n",
      "Epoch 8, Validation Loss: 0.1946\n",
      "Epoch 9, Validation Loss: 0.1916\n",
      "Epoch 10, Validation Loss: 0.1887\n",
      "Epoch 11, Validation Loss: 0.1851\n",
      "Epoch 12, Validation Loss: 0.1834\n",
      "Epoch 13, Validation Loss: 0.1824\n",
      "Epoch 14, Validation Loss: 0.1786\n",
      "Epoch 15, Validation Loss: 0.1813\n",
      "Epoch 16, Validation Loss: 0.1758\n",
      "Epoch 17, Validation Loss: 0.1731\n",
      "Epoch 18, Validation Loss: 0.1721\n",
      "Epoch 19, Validation Loss: 0.1728\n",
      "Epoch 20, Validation Loss: 0.1705\n",
      "Epoch 21, Validation Loss: 0.1695\n",
      "Epoch 22, Validation Loss: 0.1679\n",
      "Epoch 23, Validation Loss: 0.1705\n",
      "Epoch 24, Validation Loss: 0.1656\n",
      "Epoch 25, Validation Loss: 0.1751\n",
      "Epoch 26, Validation Loss: 0.1694\n",
      "Epoch 27, Validation Loss: 0.1631\n",
      "Epoch 28, Validation Loss: 0.1628\n",
      "Epoch 29, Validation Loss: 0.1635\n",
      "Epoch 30, Validation Loss: 0.1617\n",
      "Epoch 31, Validation Loss: 0.1617\n",
      "Epoch 32, Validation Loss: 0.1646\n",
      "Epoch 33, Validation Loss: 0.1610\n",
      "Epoch 34, Validation Loss: 0.1687\n",
      "Epoch 35, Validation Loss: 0.1607\n",
      "Epoch 36, Validation Loss: 0.1624\n",
      "Epoch 37, Validation Loss: 0.1611\n",
      "Epoch 38, Validation Loss: 0.1640\n",
      "Epoch 39, Validation Loss: 0.1575\n",
      "Epoch 40, Validation Loss: 0.1579\n",
      "Epoch 41, Validation Loss: 0.1562\n",
      "Epoch 42, Validation Loss: 0.1563\n",
      "Epoch 43, Validation Loss: 0.1602\n",
      "Epoch 44, Validation Loss: 0.1553\n",
      "Epoch 45, Validation Loss: 0.1576\n",
      "Epoch 46, Validation Loss: 0.1550\n",
      "Epoch 47, Validation Loss: 0.1551\n",
      "Epoch 48, Validation Loss: 0.1568\n",
      "Epoch 49, Validation Loss: 0.1577\n",
      "Epoch 50, Validation Loss: 0.1546\n",
      "Epoch 51, Validation Loss: 0.1548\n",
      "Epoch 52, Validation Loss: 0.1543\n",
      "Epoch 53, Validation Loss: 0.1592\n",
      "Epoch 54, Validation Loss: 0.1550\n",
      "Epoch 55, Validation Loss: 0.1585\n",
      "Epoch 56, Validation Loss: 0.1578\n",
      "Epoch 57, Validation Loss: 0.1552\n",
      "Epoch 58, Validation Loss: 0.1577\n",
      "Epoch 59, Validation Loss: 0.1581\n",
      "Epoch 60, Validation Loss: 0.1542\n",
      "Epoch 61, Validation Loss: 0.1544\n",
      "Epoch 62, Validation Loss: 0.1553\n",
      "Epoch 63, Validation Loss: 0.1541\n",
      "Epoch 64, Validation Loss: 0.1554\n",
      "Epoch 65, Validation Loss: 0.1565\n",
      "Epoch 66, Validation Loss: 0.1576\n",
      "Epoch 67, Validation Loss: 0.1572\n",
      "Epoch 68, Validation Loss: 0.1547\n",
      "Epoch 69, Validation Loss: 0.1559\n",
      "Epoch 70, Validation Loss: 0.1548\n",
      "Epoch 71, Validation Loss: 0.1614\n",
      "Epoch 72, Validation Loss: 0.1543\n",
      "Epoch 73, Validation Loss: 0.1584\n",
      "Epoch 74, Validation Loss: 0.1546\n",
      "Epoch 75, Validation Loss: 0.1552\n",
      "Epoch 76, Validation Loss: 0.1548\n",
      "Epoch 77, Validation Loss: 0.1547\n",
      "Epoch 78, Validation Loss: 0.1575\n",
      "Epoch 79, Validation Loss: 0.1545\n",
      "Epoch 80, Validation Loss: 0.1573\n",
      "Epoch 81, Validation Loss: 0.1546\n",
      "Epoch 82, Validation Loss: 0.1537\n",
      "Epoch 83, Validation Loss: 0.1535\n",
      "Epoch 84, Validation Loss: 0.1540\n",
      "Epoch 85, Validation Loss: 0.1585\n",
      "Epoch 86, Validation Loss: 0.1543\n",
      "Epoch 87, Validation Loss: 0.1555\n",
      "Epoch 88, Validation Loss: 0.1548\n",
      "Epoch 89, Validation Loss: 0.1571\n",
      "Epoch 90, Validation Loss: 0.1550\n",
      "Epoch 91, Validation Loss: 0.1541\n",
      "Epoch 92, Validation Loss: 0.1553\n",
      "Epoch 93, Validation Loss: 0.1552\n",
      "Epoch 94, Validation Loss: 0.1542\n",
      "Epoch 95, Validation Loss: 0.1535\n",
      "Epoch 96, Validation Loss: 0.1565\n",
      "Epoch 97, Validation Loss: 0.1564\n",
      "Epoch 98, Validation Loss: 0.1585\n",
      "Epoch 99, Validation Loss: 0.1569\n",
      "Epoch 100, Validation Loss: 0.1536\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(self.input_dim, self.hidden_dim))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        for _ in range(self.n_layers-1):\n",
    "            self.layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(self.dropout))\n",
    "        self.layers.append(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "split = int(len(X)*0.8)\n",
    "X = X.drop(columns='target')\n",
    "X_train, X_val = X[:split].values.astype(np.float64), X[split:].values.astype(np.float64)\n",
    "y_train, y_val = y[:split].values.astype(np.float64), y[split:].values.astype(np.float64)\n",
    "\n",
    "# Assuming X_train, y_train, X_val, y_val are your datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "mlp = MLP(input_dim=X_train.shape[1], hidden_dim=8, output_dim=1, n_layers=2, dropout=0.1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # For regression tasks\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    mlp.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = mlp(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.52%\n"
     ]
    }
   ],
   "source": [
    "preds = mlp(torch.from_numpy(X_test.values.astype(np.float64)).float())\n",
    "\n",
    "acc = ((preds.detach().numpy().round() == y_test.values).mean())\n",
    "print(f\"Accuracy: {acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
